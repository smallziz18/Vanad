# VANAD ML - Syst√®me de Pr√©diction de Temps d'Attente

## üìã Aper√ßu du Projet

VANAD ML est un syst√®me de machine learning avanc√© con√ßu pour pr√©dire les temps d'attente dans les syst√®mes de files d'attente. Ce projet impl√©mente une approche multi-mod√®les combinant des algorithmes d'apprentissage automatique traditionnels et des r√©seaux de neurones profonds pour optimiser les pr√©dictions.

## üéØ Objectif Principal

Pr√©dire avec pr√©cision le temps d'attente (`W`) dans un syst√®me de files d'attente en utilisant diverses caract√©ristiques temporelles et op√©rationnelles, avec pour m√©trique principale le **RRMSE (Relative Root Mean Square Error)**.

## üèóÔ∏è Architecture du Syst√®me

### Mod√®les Impl√©ment√©s

1. **Random Forest Regressor** - Mod√®le d'ensemble bas√© sur des arbres
2. **Gradient Boosting Regressor** - Boosting s√©quentiel 
3. **LightGBM** - Gradient boosting optimis√©
4. **CatBoost** - Algorithme de boosting robuste
5. **AdvancedVANADNet** - R√©seau de neurones profond personnalis√©

### R√©seau de Neurones Personnalis√©

```python
class AdvancedVANADNet(nn.Module):
    - Architecture: [512, 256, 128] neurones
    - Normalisation par batch
    - Dropout adaptatif
    - Activation ReLU
    - Initialisation Kaiming
```

## üìä Donn√©es et Pr√©paration

### Fichiers de Donn√©es
- `vanad_training_ssj.csv` - Donn√©es d'entra√Ænement
- `vanad_test_ssj.csv` - Donn√©es de test

### Variables Principales
- **Target**: `W` (temps d'attente)
- **Features de base**: `T`, `qT`, `l1`, `l2`, `l3`, `l4`, `t_hour`, `t_day_of_week`, `s`, `P_LES`, `P_Avg_LES`

### Preprocessing
- Filtrage des outliers (0 < W ‚â§ 7200)
- Suppression des valeurs extr√™mes (Q1-Q99)
- Gestion des valeurs manquantes par la m√©diane

## üîß Feature Engineering

### Features G√©n√©r√©es

#### 1. **Features de Files d'Attente**
```python
total_queue = l1 + l2 + l3 + l4
max_queue = max(l1, l2, l3, l4)
queue_std = std(l1, l2, l3, l4)
l1_ratio = l1 / total_queue
```

#### 2. **Features Temporelles**
```python
hour_sin = sin(2œÄ * t_hour / 24)
hour_cos = cos(2œÄ * t_hour / 24)
is_weekend = (t_day_of_week in [5, 6])
```

#### 3. **Features d'Interaction**
```python
queue_efficiency = qT / (total_queue + 1)
```

#### 4. **Transformations Non-lin√©aires**
```python
qT_log = log(qT + 1)
qT_sqrt = sqrt(qT)
T_log = log(T + 1)
T_sqrt = sqrt(T)
```

## üöÄ Entra√Ænement et Optimisation

### Configuration Hardware
- **Support Multi-plateforme**: CPU, CUDA, Apple Silicon (MPS)
- **Gestion M√©moire**: Nettoyage automatique, batch adaptatif
- **Monitoring**: Utilisation RAM en temps r√©el

### Hyperparam√®tres Optimis√©s

#### Mod√®les ML
```python
RandomForest: n_estimators=300, max_depth=12
GradientBoosting: n_estimators=300, max_depth=10, learning_rate=0.1
LightGBM: n_estimators=300, max_depth=10, subsample=0.8
CatBoost: iterations=300, depth=10, learning_rate=0.1
```

#### R√©seau de Neurones
```python
Optimizer: AdamW (lr=0.001, weight_decay=1e-4)
Loss: MSE
Early Stopping: patience=20, min_delta=0.0001
Batch Size: Adaptatif (max 4096)
```

## üìà M√©trique d'√âvaluation

### M√©triques Calcul√©es
- **RRMSE** (Principal): `RMSE / mean(y_true)`
- **R¬≤**: Coefficient de d√©termination
- **RMSE**: Racine de l'erreur quadratique moyenne
- **MAE**: Erreur absolue moyenne
- **Accuracy ¬±10%**: Pourcentage de pr√©dictions dans ¬±10% de la vraie valeur

### Baseline de R√©f√©rence
- **Random Forest Baseline**: RRMSE = 0.8648
- **Objectif**: Am√©liorer cette performance de r√©f√©rence

## üî¨ Techniques Avanc√©es

### 1. **Transformation Logarithmique**
```python
y_train_log = log(y_train + 1)
y_pred = exp(y_pred_log) - 1
```

### 2. **Normalisation Robuste**
- `StandardScaler` pour les r√©seaux de neurones
- `RobustScaler` pour les mod√®les ML (r√©sistant aux outliers)

### 3. **Early Stopping**
- Pr√©vention du surapprentissage
- Sauvegarde du meilleur √©tat du mod√®le

### 4. **Gradient Clipping**
```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

## üíæ Sauvegarde et Persistance

### Mod√®les Sauvegard√©s
- Top 4 mod√®les par performance RRMSE
- Format: `model_rank_name_timestamp.pkl`
- Contenu: mod√®le, scaler, m√©triques, m√©tadonn√©es

### Structure de Sauvegarde
```
saved_models/
‚îú‚îÄ‚îÄ model_1_LightGBM_20240109_143052.pkl
‚îú‚îÄ‚îÄ model_2_Neural_Network_20240109_143052.pkl
‚îú‚îÄ‚îÄ model_3_CatBoost_20240109_143052.pkl
‚îî‚îÄ‚îÄ model_4_RandomForest_20240109_143052.pkl
```

## üéØ R√©sultats Typiques

### Classement des Mod√®les (Exemple)
```
1. LightGBM:
   RRMSE: 0.7842
   R¬≤: 0.8756
   RMSE: 156.23
   MAE: 98.45
   Pr√©cision ¬±10%: 78.9%

2. Neural Network:
   RRMSE: 0.7901
   R¬≤: 0.8721
   RMSE: 157.89
   MAE: 99.12
   Pr√©cision ¬±10%: 77.8%
```

## üöÄ Utilisation

### Pr√©requis
```bash
pip install pandas numpy torch scikit-learn lightgbm catboost joblib psutil
```

### Ex√©cution
```bash
python vanad_ml.py
```

### Surveillance
- Monitoring automatique de la RAM
- Affichage des progr√®s d'entra√Ænement
- M√©triques en temps r√©el

## üîç Fonctionnalit√©s Techniques

### Gestion M√©moire
- Nettoyage automatique avec `gc.collect()`
- Vidage cache GPU/MPS
- Batch processing pour les grandes donn√©es

### Parall√©lisation
- Utilisation maximale des c≈ìurs CPU (`n_jobs=-1`)
- Support GPU complet (CUDA/MPS)

### Reproductibilit√©
- Seeds fix√©s: `torch.manual_seed(42)`, `np.random.seed(42)`
- R√©sultats reproductibles entre ex√©cutions

## üìä Monitoring et Logs

### Affichage en Temps R√©el
```
üöÄ Device: mps
üíæ RAM disponible: 32.1 GB
üìä Utilisation de toutes les donn√©es: 45623 √©chantillons
üîÑ LightGBM...
‚úÖ LightGBM: RRMSE=0.7842, R¬≤=0.8756
```

### Comparaison Performance
```
üìà Am√©lioration vs RF baseline (RRMSE 0.8648): +9.3%
```


## üîß Configuration et Personnalisation

### Param√®tres Modifiables
```python
# Taille des couches du r√©seau
hidden_layers = [512, 256, 128]

# Taux de dropout
dropout_rate = 0.3

# Patience early stopping
patience = 20

# Nombre de mod√®les √† sauvegarder
top_n = 4
```

